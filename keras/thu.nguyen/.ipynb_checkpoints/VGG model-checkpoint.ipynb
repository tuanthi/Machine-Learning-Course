{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. VGG\n",
    "### Finetuning VGG on Fruit dataset\n",
    "<img src=\"images/VGG16.png\" style=\"width:750px;height:350px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 1** </u><font color='purple'>  : **VGG16 Model** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thunguyen/miniconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                276540    \n",
      "=================================================================\n",
      "Total params: 14,991,228\n",
      "Trainable params: 276,540\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Found 9837 images belonging to 60 classes.\n",
      "{'Apple Braeburn': 0, 'Apple Golden 1': 1, 'Apple Golden 2': 2, 'Apple Golden 3': 3, 'Apple Granny Smith': 4, 'Apple Red 1': 5, 'Apple Red 2': 6, 'Apple Red 3': 7, 'Apple Red Delicious': 8, 'Apple Red Yellow': 9, 'Apricot': 10, 'Avocado': 11, 'Avocado ripe': 12, 'Banana': 13, 'Banana Red': 14, 'Cactus fruit': 15, 'Carambula': 16, 'Cherry': 17, 'Clementine': 18, 'Cocos': 19, 'Dates': 20, 'Granadilla': 21, 'Grape Pink': 22, 'Grape White': 23, 'Grape White 2': 24, 'Grapefruit Pink': 25, 'Grapefruit White': 26, 'Guava': 27, 'Huckleberry': 28, 'Kaki': 29, 'Kiwi': 30, 'Kumquats': 31, 'Lemon': 32, 'Lemon Meyer': 33, 'Limes': 34, 'Litchi': 35, 'Mandarine': 36, 'Mango': 37, 'Maracuja': 38, 'Nectarine': 39, 'Orange': 40, 'Papaya': 41, 'Passion Fruit': 42, 'Peach': 43, 'Peach Flat': 44, 'Pear': 45, 'Pear Abate': 46, 'Pear Monster': 47, 'Pear Williams': 48, 'Pepino': 49, 'Pineapple': 50, 'Pitahaya Red': 51, 'Plum': 52, 'Pomegranate': 53, 'Quince': 54, 'Raspberry': 55, 'Salak': 56, 'Strawberry': 57, 'Tamarillo': 58, 'Tangelo': 59}\n",
      "Found 29228 images belonging to 60 classes.\n",
      "Found 9837 images belonging to 60 classes.\n",
      "29228 9837 307 913\n",
      "Epoch 1/5\n",
      "913/913 [==============================] - 126s 138ms/step - loss: 10.3327 - acc: 0.3476 - val_loss: 8.8928 - val_acc: 0.4338\n",
      "Epoch 2/5\n",
      "913/913 [==============================] - 137s 150ms/step - loss: 8.6707 - acc: 0.4569 - val_loss: 8.6870 - val_acc: 0.4525\n",
      "Epoch 3/5\n",
      "913/913 [==============================] - 139s 152ms/step - loss: 7.6408 - acc: 0.5193 - val_loss: 7.2598 - val_acc: 0.5388\n",
      "Epoch 4/5\n",
      "913/913 [==============================] - 136s 149ms/step - loss: 7.0074 - acc: 0.5615 - val_loss: 7.0360 - val_acc: 0.5537\n",
      "Epoch 5/5\n",
      "913/913 [==============================] - 127s 139ms/step - loss: 6.5867 - acc: 0.5862 - val_loss: 6.7540 - val_acc: 0.5726\n",
      "Generating confusion matrix 29228\n",
      "Found 29228 images belonging to 60 classes.\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "[[486   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 492   0   0]\n",
      " [  0   0   0 ...   0 489   0]\n",
      " [ 38   0   0 ...   0   0   0]]\n",
      "Generating confusion matrix 9837\n",
      "Found 9837 images belonging to 60 classes.\n",
      "50\n",
      "100\n",
      "150\n",
      "[[163   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 164   0   0]\n",
      " [  0   0   0 ...   0 166   0]\n",
      " [ 14   0   0 ...   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[486   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 492   0   0]\n",
      " [  0   0   0 ...   0 489   0]\n",
      " [ 38   0   0 ...   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[163   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 164   0   0]\n",
      " [  0   0   0 ...   0 166   0]\n",
      " [ 14   0   0 ...   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/advanced-computer-vision\n",
    "# https://www.udemy.com/advanced-computer-vision\n",
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "# re-size all the images to this\n",
    "IMAGE_SIZE = [100, 100] # feel free to change depending on dataset\n",
    "\n",
    "# training config:\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# https://www.kaggle.com/paultimothymooney/blood-cells\n",
    "# train_path = '../large_files/blood_cell_images/TRAIN'\n",
    "# valid_path = '../large_files/blood_cell_images/TEST'\n",
    "\n",
    "# https://www.kaggle.com/moltean/fruits\n",
    "# train_path = '../large_files/fruits-360/Training'\n",
    "# valid_path = '../large_files/fruits-360/Validation'\n",
    "train_path = '../large_files/fruits-360/Training'\n",
    "valid_path = '../large_files/fruits-360/Validation'\n",
    "\n",
    "# useful for getting number of files\n",
    "image_files = glob(train_path + '/*/*.jp*g')\n",
    "valid_image_files = glob(valid_path + '/*/*.jp*g')\n",
    "\n",
    "# useful for getting number of classes\n",
    "folders = glob(train_path + '/*')\n",
    "\n",
    "\n",
    "# look at an image for fun\n",
    "# plt.imshow(image.load_img(np.random.choice(image_files)))\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add preprocessing layer to the front of VGG\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "\n",
    "# don't train existing weights\n",
    "for layer in vgg.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "# our layers - you can add more if you want\n",
    "x = Flatten()(vgg.output)\n",
    "# x = Dense(1000, activation='relu')(x)\n",
    "prediction = Dense(len(folders), activation='softmax')(x)\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "\n",
    "# view the structure of the model\n",
    "model.summary()\n",
    "\n",
    "# tell the model what cost and optimization method to use\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='rmsprop',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create an instance of ImageDataGenerator\n",
    "gen = ImageDataGenerator(\n",
    "  rotation_range=20,\n",
    "  width_shift_range=0.1,\n",
    "  height_shift_range=0.1,\n",
    "  shear_range=0.1,\n",
    "  zoom_range=0.2,\n",
    "  horizontal_flip=True,\n",
    "  vertical_flip=True,\n",
    "  preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "\n",
    "# test generator to see how it works and some other useful things\n",
    "\n",
    "# get label mapping for confusion matrix plot later\n",
    "test_gen = gen.flow_from_directory(valid_path, target_size=IMAGE_SIZE)\n",
    "print(test_gen.class_indices)\n",
    "labels = [None] * len(test_gen.class_indices)\n",
    "for k, v in test_gen.class_indices.items():\n",
    "  labels[v] = k\n",
    "\n",
    "# should be a strangely colored image (due to VGG weights being BGR)\n",
    "# for x, y in test_gen:\n",
    "#   print(\"min:\", x[0].min(), \"max:\", x[0].max())\n",
    "#   plt.title(labels[np.argmax(y[0])])\n",
    "#   plt.imshow(x[0])\n",
    "#   plt.show()\n",
    "#   break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create generators\n",
    "train_generator = gen.flow_from_directory(\n",
    "  train_path,\n",
    "  target_size=IMAGE_SIZE,\n",
    "  shuffle=True,\n",
    "  batch_size=batch_size,\n",
    ")\n",
    "valid_generator = gen.flow_from_directory(\n",
    "  valid_path,\n",
    "  target_size=IMAGE_SIZE,\n",
    "  shuffle=True,\n",
    "  batch_size=batch_size,\n",
    ")\n",
    "\n",
    "print(len(image_files), len(valid_image_files) , len(valid_image_files) // batch_size, len(image_files) // batch_size)\n",
    "# fit the model\n",
    "r = model.fit_generator(\n",
    "  train_generator,\n",
    "  validation_data=valid_generator,\n",
    "  epochs=epochs,\n",
    "  steps_per_epoch=len(image_files) // batch_size,\n",
    "  validation_steps=len(valid_image_files) // batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_confusion_matrix(data_path, N):\n",
    "  # we need to see the data in the same order\n",
    "  # for both predictions and targets\n",
    "  print(\"Generating confusion matrix\", N)\n",
    "  predictions = []\n",
    "  targets = []\n",
    "  i = 0\n",
    "  for x, y in gen.flow_from_directory(data_path, target_size=IMAGE_SIZE, shuffle=False, batch_size=batch_size * 2):\n",
    "    i += 1\n",
    "    if i % 50 == 0:\n",
    "      print(i)\n",
    "    p = model.predict(x)\n",
    "    p = np.argmax(p, axis=1)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    predictions = np.concatenate((predictions, p))\n",
    "    targets = np.concatenate((targets, y))\n",
    "    if len(targets) >= N:\n",
    "      break\n",
    "\n",
    "  cm = confusion_matrix(targets, predictions)\n",
    "  return cm\n",
    "\n",
    "\n",
    "cm = get_confusion_matrix(train_path, len(image_files))\n",
    "print(cm)\n",
    "valid_cm = get_confusion_matrix(valid_path, len(valid_image_files))\n",
    "print(valid_cm)\n",
    "\n",
    "\n",
    "# plot some data\n",
    "\n",
    "# loss\n",
    "plt.plot(r.history['loss'], label='train loss')\n",
    "plt.plot(r.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='train acc')\n",
    "plt.plot(r.history['val_acc'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "from util import plot_confusion_matrix\n",
    "plot_confusion_matrix(cm, labels, title='Train confusion matrix')\n",
    "plot_confusion_matrix(valid_cm, labels, title='Validation confusion matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
